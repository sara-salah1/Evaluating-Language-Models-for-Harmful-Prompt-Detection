# Evaluating-Language-Models-for-Harmful-Prompt-Detection

## Overview
This project focuses on evaluating and comparing the effectiveness of different language models in detecting harmful prompts using a dataset from the LLM-EvaluationHub. The primary objective is to implement and assess at least two language models, analyze their performance, and determine which model is most effective at identifying harmful content.

## Objectives
1. **Data Analysis**

- Perform exploratory analysis of the provided dataset.
- Create visualizations to understand the distribution and characteristics of harmful prompts.
2. **Model Implementation**

- Implement at least two different language models for harmful prompt detection.
- Explore various models to achieve a comprehensive comparison.
3. **Performance Comparison**

- Evaluate the performance of each model based on accuracy and effectiveness in detecting harmful prompts.
- Compare and contrast the models to identify which performs best in identifying harmful content.


## Dataset
**Source:** LLM-EvaluationHub
**Description:** The dataset contains prompts labeled as harmful or non-harmful. It is used to train and evaluate the language models for detecting harmful content
